# 测试报告

## 一、使用占锁的方式

### 1.思路

我们可以去同一个地方（`redis`，`mysql`，等）`占锁`，如果占到，就执行逻辑，否则就必须等待，直到其他线程释放锁，等待可以使用自旋的方式。

如果上述命令执行`ok`，那么该客户端（`线程`）就会获得锁

如果上述命令返回`Nil`，那么客户端可以在一段时间之后重新尝试（`自旋`），并且可以通过[DEL](http://www.redis.cn/commands/del.html)命令来释放锁。

客户端加锁之后，如果没有主动释放，会在过期时间之后自动释放。

### 2.问题

`A`客户端获得的锁（键key）已经由于过期时间到了被`redis`服务器删除，但是这个时候`A`客户端还去执行[DEL](http://www.redis.cn/commands/del.html)命令。而`B`客户端已经在`A`设置的过期时间之后重新获取了这个同样key的锁，那么`A`执行[DEL](http://www.redis.cn/commands/del.html)就会释放了`B`客户端加好的锁。

### 3.解决方法

* 不要设置固定的字符串，而是设置为随机的大字符串，可以称为token。

* 通过脚本删除指定锁的key，而不是[DEL](http://www.redis.cn/commands/del.html)命令。

  > ```lua
  > if redis.call("get",KEYS[1]) == ARGV[1]
  > then
  >     return redis.call("del",KEYS[1])
  > else
  >     return 0
  > end
  > ```

### 4.演进

#### （1）设置锁的过期时间，超时自动解锁

![Snip20201012_4](/Users/luo/Documents/开发笔记/images/Snip20201012_4.png)

```java
ValueOperations<String, String> ops = stringRedisTemplate.opsForValue();

// 2、上锁
ops.setIfAbsent(TREE_NODE_LIST_LOCK, uuid);

// 机房拉闸

// 3、设置过期时间，这样还是一样的问题，如果在执行这行代码之前，断电了，就死锁了 
stringRedisTemplate.expire("treeNodeListLock",10,TimeUnit.SECONDS);
```



#### （2）上锁和设置锁的过期时间保证原子性

![Snip20201012_3](/Users/luo/Documents/开发笔记/images/Snip20201012_3.png)

```java
				ValueOperations<String, String> ops = stringRedisTemplate.opsForValue();

        String uuid = UUID.randomUUID().toString(); // 使用 uuid 作为 token，解锁时，只有提供该 token 才能解锁，避免了 B 线程删除 A 线程的锁
        /**
         * 1、占领分布式锁
         * 不存在该 key 的时候才能设置成功
         * 通过第三步可以得知，占锁和设置过期时间，必须是一个原子操作（同步的），要放在一起
         * 可直接调用 spring 为我们提供的方法 setIfAbsent()
         * */
        Boolean treeNodeListLock = ops.setIfAbsent("锁的名字", uuid, 10, TimeUnit.SECONDS);

        if (!ObjectUtils.isEmpty(treeNodeListLock) && treeNodeListLock) {

            log.info("{}持有分布式锁",Thread.currentThread().getName());
          // 线程持有分布式锁，执行排他业务逻辑
        }
```



#### （3）解锁时保证原子性

![Snip20201012_5](/Users/luo/Documents/开发笔记/images/Snip20201012_5.png)

```java
						/**
             * 2、这个线程占用到redis了（加锁成功）
             * */
            try {

								// 执行占锁时的业务逻辑

            } finally {
                /**
                 * uuid 是抢锁的时候生成的
                 * 添加判断锁的内容是不是自己放进去的，如果是自己的锁，就能放心删除
                 * */
                if(uuid.equals(ops.get("treeNodeListLock"))){
                  /**
                   *  删除我自己的锁，但是这两个操作不是原子操作，虽然确认到是我自己的锁，
                   *  但是在发送删除操作之前，锁被更新成其他人的锁了，这样删掉的还是其他人的锁
                   *  官方的解决方式 Lua 脚本解锁（保证删锁的原子性）
                   */
                	stringRedisTemplate.delete("treeNodeListLock");
                }
              }
```

> 还存在的问题：
>
> 删锁的时候还有问题：若这个任务需要执行`30s`，而锁的有效期只有`10s`，占用锁的线程没有自动删除锁，而是锁它自己到期失效了，此时，其他的线程就自己跑进来了，但是原先正在执行任务的线程依然在继续执行，这样就相当于有多个线程同时执行操作数据库，原先的任务执行完之后，它就把锁删了，但是删除的是别人的锁，这下就又放进一个线程来操作数据库 
>
> 解决方法：
>
> 给锁自动续期，业务还没有执行完，锁就不能失效，使用`redisson`的看门狗，给锁自动续期

#### （4）redisson自动续期源码

```java
     RLock lock = redissonClient.getLock("hello-lock");

     /**
         * 1、如果我们未指定锁的超时时间，就使用 看门狗默认时间 30s commandExecutor.getConnectionManager().getCfg().getLockWatchdogTimeout();
         * 只要占锁成功就会启动一个定时任务【重新给锁设置过期时间，锁的过期时间就是看门狗的默认时间】
         * */
        lock.lock(); // 阻塞式等待，默认锁的续期时间是 30s。
        /**
         * 2、当自定义锁的时间之后，看门狗将不会自动续期锁的时间，所以，自动解锁的时间一定要大于业务的执行时间
         * 如果我们传递了锁的超时时间，就发送给 redis
         * 10s 执行一次续期
         * */
//         lock.lock(10,TimeUnit.SECONDS);
```

```java
public class RedissonLock extends RedissonExpirable implements RLock { 
  
      private RFuture<Boolean> tryAcquireOnceAsync(long waitTime, long leaseTime, TimeUnit unit, long threadId) {
        if (leaseTime != -1) {
            return tryLockInnerAsync(waitTime, leaseTime, unit, threadId, RedisCommands.EVAL_NULL_BOOLEAN); // 用户设置了锁的超时时间，会执行
        }
        
        // 用户没有指定超时时间，使用看门狗的时间，返回 Future 接口的实现类
        RFuture<Boolean> ttlRemainingFuture = tryLockInnerAsync(waitTime,
                                                    commandExecutor.getConnectionManager().getCfg().getLockWatchdogTimeout(),//超时时间就使用看门狗； lockWatchdogTimeout = 30 * 1000; 
                                                    TimeUnit.MILLISECONDS, threadId, RedisCommands.EVAL_NULL_BOOLEAN);
        // 当 Future 任务占用到锁的时候 触发这个方法
        ttlRemainingFuture.onComplete((ttlRemaining, e) -> {
            if (e != null) {
                return;
            }

            // lock acquired
            if (ttlRemaining) {
              // 获得锁，开始调度
                scheduleExpirationRenewal(threadId);
            }
        });
        return ttlRemainingFuture;
    }
  
      private void scheduleExpirationRenewal(long threadId) {
        ExpirationEntry entry = new ExpirationEntry();
        ExpirationEntry oldEntry = EXPIRATION_RENEWAL_MAP.putIfAbsent(getEntryName(), entry);
        if (oldEntry != null) {
            oldEntry.addThreadId(threadId);
        } else {
            entry.addThreadId(threadId);
            renewExpiration(); //更新过期时间
        }
    }
  
      private void renewExpiration() {
        ExpirationEntry ee = EXPIRATION_RENEWAL_MAP.get(getEntryName());
        if (ee == null) {
            return;
        }
        
        Timeout task = commandExecutor.getConnectionManager().newTimeout(new TimerTask() {
            @Override
            public void run(Timeout timeout) throws Exception {
                ExpirationEntry ent = EXPIRATION_RENEWAL_MAP.get(getEntryName());
                if (ent == null) {
                    return;
                }
                Long threadId = ent.getFirstThreadId();
                if (threadId == null) {
                    return;
                }
                
                RFuture<Boolean> future = renewExpirationAsync(threadId);//看这里,创建续期任务
                future.onComplete((res, e) -> {
                    if (e != null) {
                        log.error("Can't update lock " + getName() + " expiration", e);
                        return;
                    }
                    
                    if (res) {
                        // reschedule itself
                        renewExpiration(); //续期之后，继续调用自己来继续续期
                    }
                });
            }
        }, internalLockLeaseTime / 3, TimeUnit.MILLISECONDS);
        // internalLockLeaseTime / 3 之后，执行续期任务，看门狗时间的1/3，也就是10秒之后执行
        ee.setTimeout(task);
    }
  
      protected RFuture<Boolean> renewExpirationAsync(long threadId) {
        return evalWriteAsync(getName(), LongCodec.INSTANCE, RedisCommands.EVAL_BOOLEAN,
                "if (redis.call('hexists', KEYS[1], ARGV[2]) == 1) then " +
                        "redis.call('pexpire', KEYS[1], ARGV[1]); " +
                        "return 1; " +
                        "end; " +
                        "return 0;",
                Collections.singletonList(getName()),
                internalLockLeaseTime, getLockName(threadId));
        // 更新的超时时间 internalLockLeaseTime
        // 在构造器中         this.internalLockLeaseTime = commandExecutor.getConnectionManager().getCfg().getLockWatchdogTimeout();
				//又是看门狗时间
    }
  
  
  // 无论用户有没有设置过期时间，底层最终都调用这个方法
<T> RFuture<T> tryLockInnerAsync(long waitTime, long leaseTime, TimeUnit unit, long threadId, RedisStrictCommand<T> command) {
        internalLockLeaseTime = unit.toMillis(leaseTime);

        return evalWriteAsync(getName(), LongCodec.INSTANCE, command,
                "if (redis.call('exists', KEYS[1]) == 0) then " +
                        "redis.call('hincrby', KEYS[1], ARGV[2], 1); " +
                        "redis.call('pexpire', KEYS[1], ARGV[1]); " +
                        "return nil; " +
                        "end; " +
                        "if (redis.call('hexists', KEYS[1], ARGV[2]) == 1) then " +
                        "redis.call('hincrby', KEYS[1], ARGV[2], 1); " +
                        "redis.call('pexpire', KEYS[1], ARGV[1]); " +
                        "return nil; " +
                        "end; " +
                        "return redis.call('pttl', KEYS[1]);",
                Collections.singletonList(getName()), internalLockLeaseTime, getLockName(threadId));
    }
}
```

![Snip20201012_6](/Users/luo/Documents/开发笔记/images/Snip20201012_6.png)

### 5.简单的示例代码

```java
    /**
     * 使用 redis 的分布式锁 来使得分布式应用只有一个去查询数据库，并将查询到的结果放入 redis
     *
     * @param ops
     * @return
     */
    private TreeNode<CategoryEntity> writeToRedisWithDistributedLock(ValueOperations<String, String> ops) {

        String uuid = UUID.randomUUID().toString();
        /**
         * 1、占领分布式锁
         * 不存在该 key 的时候才能设置成功
         * 通过第三步可以得知，占锁和设置过期时间，必须是一个原子操作（同步的），要放在一起
         * */
        Boolean treeNodeListLock = ops.setIfAbsent(TREE_NODE_LIST_LOCK, uuid, 10, TimeUnit.SECONDS);



        TreeNode<CategoryEntity> result = null;

        if (!ObjectUtils.isEmpty(treeNodeListLock) && treeNodeListLock) {

            log.info("{}持有分布式锁",Thread.currentThread().getName());
            /**
             * 2、这个线程占用到redis了（加锁成功）
             * */
            try {

                result = queryCategoryAndSetIntoRedis(ops);

            } finally {
                /**
                 * 添加判断锁的内容是不是自己放进去的，如果是自己的锁，就能放心删除
                 * */
                //if(uuid.equals(ops.get("treeNodeListLock"))){
                /**
                 *  删除我自己的锁，但是这两个操作不是原子操作，虽然确认到是我自己的锁，
                 *  但是在发送删除操作之前，锁被更新成其他人的锁了，这样删掉的还是其他人的锁
                 *  官方的解决方式 Lua 脚本解锁
                 */
                //   stringRedisTemplate.delete("treeNodeListLock");
                //}
                String luaScript = "if redis.call('get',KEYS[1]) == ARGV[1] then return redis.call('del',KEYS[1]) else return 0 end";

                /**
                 * 127.0.0.1:6379> del aa
                 * (integer) 1
                 * 127.0.0.1:6379> del aa
                 * (integer) 0
                 * redis 删除成功失败，都会返回 integer
                 */
                RedisScript<Long> script = new DefaultRedisScript<>(luaScript, Long.class); // integer 为 脚本返回的类型

                /**
                 * 使用 lua 脚本使得，查询和删除是一个原子操作
                 *  lua 脚本中的 KEYS[1] 就是 Collections.singletonList("treeNodeListLock").get(0)
                 *  lua 脚本中的 ARGV[1] 就是 uuid
                 *
                 *  如果使用 lettuce-core 执行脚本的时候，还会报异常，所以还是切回 jedis
                 * */
                stringRedisTemplate.execute(script, Collections.singletonList(TREE_NODE_LIST_LOCK), uuid);
            }
            return result;
        } else {
            /**
             * 加锁失败，要重试
             * */
/*            do { // 这样，如果拿到锁的线程断电了，如果没有新线程来获取锁，这样所有的 while 中，将没有线程去顶替，会直到栈溢出
                result = getTreeNodeListFromCache(ops);

                System.out.println(Thread.currentThread().getName() + "自旋等待结果");

                try {
                    TimeUnit.MILLISECONDS.sleep(100);
                } catch (InterruptedException e) {
                    e.printStackTrace();
                }

            } while (ObjectUtils.isEmpty(result));*/

            log.info("{}自旋",Thread.currentThread().getName());

            // 不睡觉，就是想栈溢出
            try {
                TimeUnit.SECONDS.sleep(1);
            } catch (InterruptedException e) {
                e.printStackTrace();
            }
            /**
             * 加锁失败，自旋，注意自旋调用的不是本方法，而是在本方法的基础上还要查一遍 redis 查看是否有数据
             * 所以就调用父方法，它刚好执行了这个步骤，
             * 而不是上来就去抢占锁
             * */
            return treeNodeList();
        }
    }
```





## 二、Lua 脚本方式

### 1.测试结果

> `sku1 = 100000件`,`sku2 = 200000件`，每个用户下单都包含`1`件`sku1`，`2`件`sku2`，刚好卖完

| Label        | # Samples | Average | Median | 90% Line | 95% Line | 99% Line | Min  | Max  | Error % | Throughput | Received KB/sec | Sent KB/sec |
| ------------ | --------- | ------- | ------ | -------- | -------- | -------- | ---- | ---- | ------- | ---------- | --------------- | ----------- |
| HTTP Request | 100000    | 109     | 103    | 150      | 167      | 204      | 7    | 4935 | 0.00%   | 442.5895   | 72.99           | 0           |
| TOTAL        | 100000    | 109     | 103    | 150      | 167      | 204      | 7    | 4935 | 0.00%   | 442.5895   | 72.99           | 0           |

## 

>`sku1 = 100000件`,`sku2 = 100000件`，每个用户下单都包含`1`件`sku1`，`2`件`sku2`，用户先下单`sku1`，再下单`sku2`

| Label        | # Samples | Average | Median | 90% Line | 95% Line | 99% Line | Min  | Max  | Error % | Throughput | Received KB/sec | Sent KB/sec |
| ------------ | --------- | ------- | ------ | -------- | -------- | -------- | ---- | ---- | ------- | ---------- | --------------- | ----------- |
| HTTP Request | 100000    | 114     | 111    | 161      | 179      | 222      | 10   | 1166 | 0.00%   | 421.15016  | 138.81          | 0           |
| TOTAL        | 100000    | 114     | 111    | 161      | 179      | 222      | 10   | 1166 | 0.00%   | 421.15016  | 138.81          | 0           |

